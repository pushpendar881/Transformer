# Transformer from Scratch

This project implements a Transformer model from scratch using Python and PyTorch.

## Features
- Implements Transformer architecture with self-attention and positional encoding.
- Uses PyTorch for deep learning operations.
- Can be trained on text-based tasks like translation or text classification.
- Supports multi-head attention for capturing complex dependencies.
- Includes layer normalization and dropout for stability.

## Model Components
1. **Embedding Layer**: Converts input tokens into dense vectors.
2. **Positional Encoding**: Adds positional information to embeddings.
3. **Multi-Head Self-Attention**: Captures dependencies across words.
4. **Feedforward Network**: Applies non-linearity after attention.
5. **Layer Normalization & Dropout**: Helps with model stability.
6. **Encoder & Decoder Stacks**: Consists of multiple Transformer layers.

##Dataset use is 
Helsinki-NLP/opus_books
https://huggingface.co/datasets/Helsinki-NLP/opus_books





